---
title: Segments
description: Understanding segments for parallel message processing and ordering guarantees in Fluxzero
slug: docs/reference/segment
sidebar:
   order: 70
---

import { Tabs, TabItem, Card, CardGrid, Aside } from '@astrojs/starlight/components';

**Segments** in Fluxzero are processing units that enable both high parallelism and strict ordering guarantees. They use consistent hashing to ensure that messages about the same entity are always handled by the same consumer thread in the correct order, while allowing different entities to be processed concurrently across segments.

<Aside type="tip" title="Real-world analogy">
Think of segments like specialized work stations in a smart home factory. Each station (segment) handles all tasks for specific devicesâ€”so all "living room light" operations go to Station A, while "kitchen thermostat" operations go to Station B. This ensures each device's operations happen in the right order, while multiple devices can be worked on simultaneously across different stations.
</Aside>

## Key Concepts

<CardGrid>
<Card title="Consistent Assignment" icon="setting">
Messages are assigned to segments using consistent hashing based on routing keys, ensuring the same entity always routes to the same segment for ordering guarantees.
</Card>

<Card title="Parallel Processing" icon="rocket">
Different segments can process messages concurrently across multiple consumer threads, providing horizontal scalability while maintaining per-entity consistency.
</Card>

<Card title="Data Locality" icon="document">
Related messages processed in the same segment benefit from cache locality, allowing efficient in-memory state management and improved performance.
</Card>

<Card title="Load Distribution" icon="list-format">
Segments are automatically distributed across available consumer threads and instances, providing automatic load balancing and fault tolerance.
</Card>
</CardGrid>

## Related Concepts

- **[Routing Keys](/docs/reference/routing-key)** - How messages are assigned to specific segments
- **[Handlers](/docs/reference/handler)** - Processing components that run within segments
- **[Aggregates](/docs/reference/aggregate)** - How entity IDs automatically provide segment routing
- **[Message Log](/docs/reference/message-log)** - Storage system that segments read messages from
- **[Commands](/docs/reference/command)** - Messages that are distributed across segments for processing
- **[Events](/docs/reference/event)** - Messages processed by segments to maintain consistency
- **[Tracker](/docs/reference/tracker)** - Components that manage segment assignment and processing

## Segment lifecycle

```mermaid
graph LR
    ML[Message Log] --> SA[Segment Assignment]
    SA --> T[Tracker]
    T --> H[Handler]
```

Messages are written to the message log, then assigned to segments based on routing keys. Each segment is processed by a tracker (a consumer thread), which invokes the appropriate handler to process messages in order.

## Default configuration and limits

By default, Fluxzero uses **128 segments** to balance parallelism and ordering guarantees. These segments are evenly split across available trackers to distribute load efficiently. The `ignoreSegment` configuration, when set to `true`, means that each tracker receives all messages regardless of segment assignment. This allows filtering and routing to be applied at the client or handler level using custom routing keys rather than relying on segment-based filtering. The `singleTracker` option can be enabled to force all segments to be managed by a single tracker instance, simplifying ordering at the cost of scalability.

Example pattern using custom routing keys with `ignoreSegment` enabled:

<Tabs>
  <TabItem value="java" label="Java">

```java
@HandleCommand
@RoutingKey("paymentId")
void handle(SubmitOrder command) {
    // Handler logic here, routing messages by paymentId instead of segment
}
```

  </TabItem>
  <TabItem value="kotlin" label="Kotlin">

```kotlin
@HandleCommand
@RoutingKey("paymentId")
fun handle(command: SubmitOrder) {
    // Handler logic here, routing messages by paymentId instead of segment
}
```

  </TabItem>
</Tabs>

## Ordering guarantees

Ordering is guaranteed **per routing key** within a segment, ensuring that messages for the same entity are processed in the order they were emitted. However, there is **no global ordering** across different routing keys or segments. To achieve global ordering, all messages must share the same routing key or be processed by a single segment, which may impact throughput and scalability.

## Customization

Segments can be customized using the `@RoutingKey` annotation to specify the routing key for messages explicitly. The `ignoreSegment` flag can be used for messages that do not require ordering guarantees or segment assignment. Designing your system involves trade-offs between maximizing parallelism (more segments) and maintaining strict ordering (fewer segments or single segment). Careful tuning of these parameters helps optimize performance and consistency.

## Operational considerations

- **Hot Segments:** Some segments may receive disproportionately high traffic ("hot spots"), causing uneven load; consider sharding or adjusting routing keys to mitigate.
- **Monitoring Skew:** Regularly monitor segment load and processing latency to detect imbalances or bottlenecks.
- **Best Practices:** Use consistent and meaningful routing keys, avoid excessive segment counts that may cause overhead, and leverage `ignoreSegment` for messages where ordering is not critical.

## Real-world examples

- **SaaS Multi-tenant:** Each tenant's data is routed to a specific segment to ensure tenant-specific ordering while enabling parallel processing across tenants.
- **IoT Device Management:** Devices are assigned routing keys based on device IDs, allowing concurrent processing of different devices while preserving event order per device.
- **Banking Transactions:** Account IDs are used as routing keys to guarantee transaction ordering per account, preventing race conditions and ensuring consistency.
